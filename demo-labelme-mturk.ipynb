{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot as plt \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import sys\n",
    "import deepdish as dd\n",
    "\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator\n",
    "\n",
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 30\n",
    "DATA_PATH = \"/Users/diment/Downloads/LabelMe/prepared/\"\n",
    "N_CLASSES = 8\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def load_data(filename):\n",
    "    if sys.version_info[0] < 3:\n",
    "        f = open(filename)\n",
    "        data = np.load(f)\n",
    "        f.close()\n",
    "    else:\n",
    "        data = dd.io.load(filename.replace('.npy', '.h5'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(10000, 4, 4, 512)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "\n",
      "Loading AMT data...\n",
      "(10000, 59)\n",
      "\n",
      "N_CLASSES: 8\n",
      "N_ANNOT: 59\n",
      "\n",
      "Loading test data...\n",
      "(1188, 4, 4, 512)\n",
      "(1188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading train data...\")\n",
    "\n",
    "# images processed by VGG16\n",
    "data_train_vgg16 = load_data(DATA_PATH+\"data_train_vgg16.npy\")\n",
    "print(data_train_vgg16.shape)\n",
    "\n",
    "# ground truth labels\n",
    "labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "print(labels_train.shape)\n",
    "\n",
    "# labels obtained from majority voting\n",
    "labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "print(labels_train_mv.shape)\n",
    "\n",
    "# labels obtained by using the approach by Dawid and Skene\n",
    "labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "print(labels_train_ds.shape)\n",
    "\n",
    "# data from Amazon Mechanical Turk\n",
    "print(\"\\nLoading AMT data...\")\n",
    "answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "print(answers.shape)\n",
    "N_ANNOT = answers.shape[1]\n",
    "print(\"\\nN_CLASSES:\", N_CLASSES)\n",
    "print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "# load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "\n",
    "# images processed by VGG16\n",
    "data_test_vgg16 = load_data(DATA_PATH+\"data_test_vgg16.npy\")\n",
    "print(data_test_vgg16.shape)\n",
    "\n",
    "# test labels\n",
    "labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save h5 versions for python3 compatibility: run this under python2\n",
    "if False:\n",
    "    import deepdish as dd\n",
    "    for filename in ['data_train_vgg16.npy', 'labels_train.npy', 'labels_train_mv.npy', 'labels_train_DS.npy', 'answers.npy', 'data_test_vgg16.npy', 'labels_test.npy']:\n",
    "        inpath = DATA_PATH + filename\n",
    "        outpath = DATA_PATH + filename.replace('.npy', '.h5')\n",
    "        data = load_data(inpath)\n",
    "        dd.io.save(outpath, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting to one-hot encoding...\n",
      "(10000, 8)\n",
      "(10000, 8)\n",
      "(10000, 8)\n",
      "(1188, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 8, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nConverting to one-hot encoding...\")\n",
    "labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "print(labels_train_bin.shape)\n",
    "labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "print(labels_train_mv_bin.shape)\n",
    "labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "print(labels_train_ds_bin.shape)\n",
    "labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "print(labels_test_bin.shape)\n",
    "\n",
    "answers_bin_missings = []\n",
    "for i in range(len(answers)):\n",
    "    row = []\n",
    "    for r in range(N_ANNOT):\n",
    "        if answers[i,r] == -1:\n",
    "            row.append(-1 * np.ones(N_CLASSES))\n",
    "        else:\n",
    "            row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "    answers_bin_missings.append(row)\n",
    "answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "answers_bin_missings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the base deep learning model\n",
    "\n",
    "Here we shall use features representation produced by the VGG16 network as the input. Our base model is then simply composed by one densely-connected layer with 128 hidden units and an output dense layer. We use 50% dropout between the two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model():\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=data_train_vgg16.shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation(\"softmax\"))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary function for evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, test_labels):\n",
    "    # testset accuracy\n",
    "    preds_test = model.predict(test_data)\n",
    "    preds_test_num = np.argmax(preds_test, axis=1)\n",
    "    accuracy_test = 1.0*np.sum(preds_test_num == test_labels) / len(test_labels)\n",
    "\n",
    "    return accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on the true labels (ground truth) and evaluate on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 2s - loss: 0.5070\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.1990\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.1242\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.0821\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.0639\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.0504\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.0431\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.0407\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.0337\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.0301\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.0373\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.0413\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.0280\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.0275\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.0287\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.0464\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.0277\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.0295\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.0210\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.0188\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.0240\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.0295\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.0360\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.0351\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.0304\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.0230\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.0272\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.0232\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.0201\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.0197\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.0254\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.0313\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.0248\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0174\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0207\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.0272\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0171\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0230\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0167\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0143\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.0142\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0197\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0140\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0193\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0284\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0273\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0244\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0119\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0159\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb20d2feb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "model.fit(data_train_vgg16, labels_train_bin, epochs=N_EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Test: 0.908\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = eval_model(model, data_test_vgg16, labels_test)\n",
    "print(\"Accuracy: Test: %.3f\" % (accuracy_test,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on the output of majority voting and evaluate on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 2s - loss: 0.8617\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.5076\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3659\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.2777\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.2117\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.1678\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.1378\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.1323\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.1161\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.0911\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.0837\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.0865\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.0957\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.0752\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.0701\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.0694\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.0840\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.0702\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.0723\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.0650\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.0554\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.0560\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.0656\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.0652\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.0539\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.0618\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.0630\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.0467\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.0617\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.0516\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.0479\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.0521\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.0593\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0544\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0528\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.0579\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0625\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0493\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0521\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0580\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.0530\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0527\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0429\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0362\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0426\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0512\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0494\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0427\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.0463\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb37600b38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "model.fit(data_train_vgg16, labels_train_mv_bin, epochs=N_EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Test: 0.774\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = eval_model(model, data_test_vgg16, labels_test)\n",
    "print(\"Accuracy: Test: %.3f\" % (accuracy_test,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on the output of Dawid & Skene [1] and evaluate on testset\n",
    "\n",
    "[1] Dawid, A.P. and Skene, A.M., 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics, pp.20-28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 2s - loss: 0.8893\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.5147\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.3767\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.2873\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.2247\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.1860\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.1540\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.1347\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.1304\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.1096\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.0868\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.0852\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.0790\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.0768\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.0694\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.0624\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.0837\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.0765\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.0801\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.0743\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.0603\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.0545\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.0656\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.0541\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.0689\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.0605\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.0607\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.0646\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.0556\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.0564\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.0621\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.0533\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.0426\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0456\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0505\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.0496\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0505\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0574\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0521\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0737\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.0477\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.0477\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0363\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0425\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0374\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0421\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0458\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0435\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0379\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb375ec9b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "model.fit(data_train_vgg16, labels_train_ds_bin, epochs=N_EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Test: 0.806\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = eval_model(model, data_test_vgg16, labels_test)\n",
    "print(\"Accuracy: Test: %.3f\" % (accuracy_test,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using EM approach and evaluate on testset\n",
    "\n",
    "The CrowdsCategoricalAggregator class acts as a wrapper for the base model that computed the EM steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "E-step\n",
      "Adjusted ground truth accuracy: 0.769\n",
      "M-step\n",
      "loss: 0.8985816038131714\n",
      "Epoch: 2\n",
      "E-step\n",
      "Adjusted ground truth accuracy: 0.8478\n",
      "M-step\n",
      "loss: 0.09465183200836182\n",
      "Epoch: 3\n",
      "E-step\n",
      "Adjusted ground truth accuracy: 0.8467\n",
      "M-step\n",
      "loss: 0.08496349008083344\n",
      "Epoch: 4\n",
      "E-step\n",
      "Adjusted ground truth accuracy: 0.8568\n",
      "M-step\n",
      "loss: 0.07329484004974365\n",
      "Epoch: 5\n",
      "E-step\n",
      "Adjusted ground truth accuracy: 0.8599\n",
      "M-step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f4c39ca8d0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# M-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrowds_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/CrowdLayer/crowd_layer/crowd_aggregators.py\u001b[0m in \u001b[0;36mm_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M-step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_truth_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_truth_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/unienv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/unienv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/unienv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/unienv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/unienv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "crowds_agg = CrowdsCategoricalAggregator(model, data_train_vgg16, answers, batch_size=BATCH_SIZE)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    \n",
    "    # E-step\n",
    "    ground_truth_est = crowds_agg.e_step()\n",
    "    print(\"Adjusted ground truth accuracy:\", 1.0*np.sum(np.argmax(ground_truth_est, axis=1) == labels_train) / len(labels_train))\n",
    "    \n",
    "    # M-step\n",
    "    model, pi = crowds_agg.m_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = eval_model(model, data_test_vgg16, labels_test)\n",
    "print(\"Accuracy: Test: %.3f\" % (accuracy_test,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using proposed DL-MW approach and evaluate on testset\n",
    "\n",
    "We start by adding a new layer (CrowdsClassification) on top of our neural network. We then require a special loss (MaskedMultiCrossEntropy) to handle the missing labels from some of the annotators (encoded as \"-1\").\n",
    "\n",
    "Notice how the training is faster then the EM approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 0.0712\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.0621\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.0572\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.0527\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.0494\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.0459\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.0431\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.0402\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.0379\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.0363\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.0345\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.0328\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.0310\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.0297\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.0287\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.0281\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.0270\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.0259\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.0251\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.0244\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.0235\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.0230\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.0225\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.0222\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.0217\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.0207\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.0207\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.0204\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.0198\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.0194\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.0194\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.0188\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.0182\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0180\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0180\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.0175\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0172\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.0171\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0179\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0172\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.0168\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0164\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0162\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0164\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0159\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0158\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0152\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0152\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0152\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.0149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb430249e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "\n",
    "# add crowds layer on top of the base model\n",
    "model.add(CrowdsClassification(N_CLASSES, N_ANNOT, conn_type=\"MW\"))\n",
    "\n",
    "# instantiate specialized masked loss to handle missing answers\n",
    "loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "# compile model with masked loss and train\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.fit(data_train_vgg16, answers_bin_missings, epochs=N_EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating our model, we need to remove the crowds layer used during training in order to expose the aggregation (bottleneck) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Test: 0.824\n"
     ]
    }
   ],
   "source": [
    "# save weights from crowds layer for later\n",
    "weights = model.layers[5].get_weights()\n",
    "\n",
    "# remove crowds layer before making predictions\n",
    "model.pop() \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "accuracy_test = eval_model(model, data_test_vgg16, labels_test)\n",
    "print(\"Accuracy: Test: %.3f\" % (accuracy_test,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the weights learned by the crowds layer for each annotator with their true confution matrices\n",
    "\n",
    "First, compute true confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mats = np.zeros((N_CLASSES,N_CLASSES,N_ANNOT)) + 0.00000001\n",
    "num_answers = np.zeros((N_CLASSES,N_ANNOT)) + (0.00000001 * N_CLASSES)\n",
    "for i in range(len(answers)):\n",
    "    for r in range(N_ANNOT):\n",
    "        if answers[i][r] != -1:\n",
    "            num_answers[labels_train[i],r] += 1\n",
    "            conf_mats[labels_train[i],answers[i][r],r] += 1\n",
    "for r in range(N_ANNOT):\n",
    "    for c in range(N_CLASSES):\n",
    "        for c2 in range(N_CLASSES):\n",
    "            conf_mats[c,c2,r] = conf_mats[c,c2,r] / num_answers[c,r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function that make a visual comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_conf_mats(true_conf_mat, weights):\n",
    "    # normalize weights matrix between 0 and 1\n",
    "    w_mat = (np.transpose(weights) + np.abs(weights.min()))\n",
    "    w_mat = w_mat / w_mat.max()\n",
    "    \n",
    "    sp1 = plt.subplot(1,2,1)\n",
    "    plt.imshow(true_conf_mat, interpolation='nearest', cmap=cm.YlOrRd)\n",
    "    plt.title(\"True\")\n",
    "\n",
    "    sp = plt.subplot(1,2,2)\n",
    "    plt.imshow(w_mat, interpolation='nearest', cmap=cm.YlOrRd)\n",
    "    plt.title(\"Estimated\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make comparison for various annotators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,1], weights[0][:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,2], weights[0][:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,9], weights[0][:,:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,20], weights[0][:,:,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,23], weights[0][:,:,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,30], weights[0][:,:,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,36], weights[0][:,:,36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,39], weights[0][:,:,39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,45], weights[0][:,:,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,56], weights[0][:,:,56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_conf_mats(conf_mats[:,:,58], weights[0][:,:,58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
